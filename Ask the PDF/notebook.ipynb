{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3mxmuywW_BA"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this project, we will develop an application called \"Ask the PDF\", which utilizes [LangChain](https://www.langchain.com/) and the [OpenAI API](https://openai.com/blog/openai-api) to provide answers based on the content of a PDF document. This application harnesses the capabilities of OpenAI to respond to questions using the data available in the PDF. Its biggest advantage is that it won't [hallucinate](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)) if it doesn't know the answer. Instead, it relies solely on the information contained within the document to formulate its responses.\n",
        "\n",
        "More documentation on how to ask questions related to the contents of documents can be found [here](https://python.langchain.com/docs/use_cases/question_answering/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-446G5qBZbyJ"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "In this section, we will install the necesarry libraries and download an example PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mk-tND9bZet1"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2 -q\n",
        "!pip install langchain -q\n",
        "!pip install faiss-cpu -q\n",
        "!pip install openai -q\n",
        "!pip install python-dotenv -q\n",
        "!pip install sentence_transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbmMM_ndZh5-",
        "outputId": "80f418c5-71a7-41e8-d19f-3373e003eef3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5552060"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# Download example PDF\n",
        "URL = \"https://www.ipcc.ch/report/ar6/syr/downloads/report/IPCC_AR6_SYR_SPM.pdf\"\n",
        "doc_to_download = requests.get(URL)\n",
        "# Save example PDF locally\n",
        "pdf_file = open(\"IPCC_AR6_SYR_SPM.pdf\", \"wb\")\n",
        "pdf_file.write(doc_to_download.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Gj8-J3aJXh"
      },
      "source": [
        "# Read PDF File\n",
        "\n",
        "In this section, we will load the downloaded PDF and read its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a2vUuilMacX_"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pfcaB6yQagtv"
      },
      "outputs": [],
      "source": [
        "# Load PDF file\n",
        "pdf_file_obj = open('IPCC_AR6_SYR_SPM.pdf', 'rb')\n",
        "pdf_reader = PdfReader(pdf_file_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pmyHTwSgak_7"
      },
      "outputs": [],
      "source": [
        "# Store contents of PDF file in new variable 'text'\n",
        "text = \"\"\n",
        "for page in pdf_reader.pages:\n",
        "  text += page.extract_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "kqjuyf5yaq8j",
        "outputId": "185efe02-f2d2-4771-d565-4655f5d11452"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' the IPCC Sixth Assessment Report (AR6) summarises the state of knowledge of climate change, \\nits widespread impacts and risks, and climate change mitigation and adaptation. It integrates the main findings of the Sixth \\nAssessment Report (AR6) based on contributions from the three Working Groups1, and the three Special Reports2. The summary \\nfor Policymakers (SPM) is structured in three parts: SPM.A Current Status and Trends, SPM.B Future Climate Change, Risks, and \\nLong-Term Responses, and SPM.C Responses in the Near Term3. \\nThis report recognizes the interdependence of climate, ecosystems and biodiversity, and human societies; the value of diverse \\nforms of knowledge; and the close linkages between climate change adaptation, mitigation, ecosystem health, human well-being \\nand sustainable development, and reflects the increasing diversity of actors involved in climate action. \\nBased on scientific understanding, key findings can be formulated as statements of fact or associated with an assessed level of \\nconfidence using the IPCC calibrated language4. \\xa0\\n1 The three Working Group contributions to AR6 are: AR6 Climate Change 2021: The Physical Science Basis; AR6 Climate Change 2022: Impacts, Adaptation \\nand Vulnerability; and AR6 Climate Change 2022: Mitigation of Climate Change. Their assessments cover scientific literature accepted for publication \\nrespectively by 31 January 2021, 1 September 2021 and 11 October 2021.\\n2 The three Special Reports are: Global Warming of 1.5°C (2018): an IPCC Special Report on the impacts of global warming of 1.5°C above pre-industrial \\nlevels and related global greenhouse gas emission pathways, in the context of strengthening the global response to the threat of climate change, sustainable \\ndevelopment, and efforts to eradicate poverty (SR1.5); Climate Change and Land (2019): an IPCC Special Report on climate change, desertification, land \\ndegradation, sustainable land management, food security, and greenhouse gas fluxes in terrestria'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Print some of the data stored in 'text'\n",
        "text[8000:10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq3iX76FbYlD"
      },
      "source": [
        "# Create Chunks\n",
        "\n",
        "In this section, we will split the content of the document (already stored in the variable \"text\") into chunks of equal size. These chunks will serve as the context that we provide to the OpenAI API for answering our questions. This segmentation is necessary because OpenAI has a limitation on the input size and cannot process the entire document simultaneously. Furthermore, supplying the entire document to OpenAI for each question would significantly escalate our costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D9PecMQXc2bU"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oT3bek9YdSHQ"
      },
      "outputs": [],
      "source": [
        "# Create text splitter object\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap = 100,\n",
        "    length_function = len\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w3WIj5UydT10"
      },
      "outputs": [],
      "source": [
        "# Split text into chunks\n",
        "chunks = text_splitter.split_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9ElL9OEdVMJ",
        "outputId": "556d4202-7e4b-4bab-d7e9-690696c4800a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Number of resulting chunks\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ulctmntjdWDZ",
        "outputId": "86cd0dc0-4507-4d06-ce80-db6eb0fb4d30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'respectively by 31 January 2021, 1 September 2021 and 11 October 2021.\\n2 The three Special Reports are: Global Warming of 1.5°C (2018): an IPCC Special Report on the impacts of global warming of 1.5°C above pre-industrial \\nlevels and related global greenhouse gas emission pathways, in the context of strengthening the global response to the threat of climate change, sustainable \\ndevelopment, and efforts to eradicate poverty (SR1.5); Climate Change and Land (2019): an IPCC Special Report on climate change, desertification, land \\ndegradation, sustainable land management, food security, and greenhouse gas fluxes in terrestrial ecosystems (SRCCL); and The Ocean and Cryosphere in'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Print a random chunk\n",
        "chunks[13]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMKQPFk4d0cB"
      },
      "source": [
        "# Create Embeddings\n",
        "\n",
        "In this section, we will generate embeddings from the chunks of text previously created. [Embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture) serve as vector representations of data, encompassing words, phrases, or documents as numbers. They play a crucial role in NLP tasks, enabling us to input information into the model. Models cannot directly process human-readable text; rather, they rely on embeddings to interpret and work with the data effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "raJaL2lBgIiC"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKn1eILQh7Tg"
      },
      "source": [
        "Instead of using OpenAI to creathe these embeddings, we will be using an open-source alternative from LangChain.\n",
        "\n",
        "Thee following are two pre-trained models from the [\"sentence-transformers\"](https://huggingface.co/sentence-transformers) library, which is a library focused on sentence and text embeddings.\n",
        "\n",
        "*   'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' - of size 420 MB\n",
        "*   'sentence-transformers/paraphrase-multilingual-mpnet-base-v2' - of size 970 MB\n",
        "\n",
        "We will be using the lighter option.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GxXViWSvjYCE"
      },
      "outputs": [],
      "source": [
        "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "El5jO6ibjbtZ"
      },
      "outputs": [],
      "source": [
        "# Example: Creating embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "sentence_embeddings = model.encode(\"Example sentence.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeCL80gbkuck",
        "outputId": "3e211c41-51b5-4f9b-9477-cd2ae93edcd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(sentence_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GddqtKMOSlfr"
      },
      "source": [
        "Next, we will create embeddings for the whole document. We will be using [FAISS](https://faiss.ai/index.html) (short for Facebook AI Similarity Search), a library that provides efficient algorithms to quickly search and cluster embedding vectors.\n",
        "\n",
        "First, we will create these embeddings, and then we will search the document for sections with content related to the question being asked. This will be the context provided to OpenAI to answer the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pp_TWHvolR5c"
      },
      "outputs": [],
      "source": [
        "# Creating embeddings for the whole document\n",
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xuktYztwTBFp"
      },
      "outputs": [],
      "source": [
        "knowledge_base = FAISS.from_texts(chunks, embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MXhVxG0_TJPE"
      },
      "outputs": [],
      "source": [
        "# Search for similar text in the document\n",
        "question = \"How will continued emissions affect climate change?\"\n",
        "context = knowledge_base.similarity_search(question, top_k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TLjX-G6T0N5",
        "outputId": "8e51aa85-fdab-42b1-d71a-6655a7b3f897"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='sectors and communities ( high confidence ). {3.4, 4.2, 4.4, 4.5, 4.7, 4.8 } (Figure SPM.6 )\\nC.1.3 Continued emissions will further affect all major climate system components, and many changes will be irreversible on \\ncentennial to millennial time scales and become larger with increasing global warming. Without urgent, effective, and \\nequitable mitigation and adaptation actions, climate change increasingly threatens ecosystems, biodiversity, and the \\nlivelihoods, health and well-being of current and future generations.  (high confidence ) {3.1.3, 3.3.3, 3.4.1, Figure 3.4, \\n4.1, 4.2, 4.3, 4.4 } (Figure SPM.1, Figure SPM.6 )25'),\n",
              " Document(page_content='reductions.  Targeted reductions of air pollutant emissions lead to more rapid improvements in air quality within years \\ncompared to reductions in GHG emissions only, but in the long term, further improvements are projected in scenarios \\nthat combine efforts to reduce air pollutants as well as GHG emissions33. (high confidence ) {3.1.1 } (Box SPM.1 )\\nB.1.3 Continued emissions will further affect all major climate system components. With every additional increment of global \\nwarming, changes in extremes continue to become larger. Continued global warming is projected to further intensify \\nthe global water cycle, including its variability, global monsoon precipitation, and very wet and very dry weather and'),\n",
              " Document(page_content='unequally distributed across systems, regions and sectors. Economic damages from climate change have been detected \\nin climate-exposed sectors, such as agriculture, forestry, fishery, energy, and tourism. Individual livelihoods have been \\naffected through, for example, destruction of homes and infrastructure, and loss of property and income, human health \\nand food security, with adverse effects on gender and social equity.  (high confidence ) {2.1.2 } (Figure SPM.1 )\\nA.2.7 In urban areas, observed climate change has caused adverse impacts on human health, livelihoods and key infrastructure. \\nHot extremes have intensified in cities. Urban infrastructure, including transportation, water, sanitation and energy'),\n",
              " Document(page_content='hotter and different world depends on choices now and in the near term\\nFuture emissions \\nscenarios:b) Impacts are driven by changes in multiple physical climate \\nconditions, which are increasingly attributed to human inﬂuence\\nAttribution of observed physical climate changes to human inﬂuence:\\nVirtually certain\\nIncrease \\nin hot \\nextremes Upper \\nocean\\nacidiﬁcationpHLikely\\nIncrease \\nin heavy \\nprecipitationVery likely\\nGlobal sea\\nlevel riseGlacier\\nretreatMedium conﬁdence\\nIncrease in \\ncompound\\nﬂoodingIncrease in \\nagricultural \\n& ecological \\ndroughtIncrease \\nin ﬁre\\nweather8\\nSummary for Policymakers\\nSummary for Policymakersrepresentative generations (born in 1950, 1980 and 2020). Future projections (2021–2100) of changes in global surface temperature are shown for very low')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFa_q03gUD6W"
      },
      "source": [
        "# Ask the Document\n",
        "\n",
        "In this section, we will provide OpenAI with context information and a question to answer based on this context. We will be using the [ChatOpenAI](https://python.langchain.com/docs/integrations/chat/openai) library from LangChain to make these calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zRIuVGW2T0xF"
      },
      "outputs": [],
      "source": [
        "# Load OpenAI API Key\n",
        "\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "L3iW4tylVQWT"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aqY-LLINVRKf"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
        "chain = load_qa_chain(llm, chain_type='stuff')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for similar text in the document\n",
        "question = \"How will continued emissions affect climate change?\"\n",
        "docs = knowledge_base.similarity_search(question, top_k=3)\n",
        "\n",
        "# Use similar text to give ChatGPT some context\n",
        "answer = chain.run(input_documents=docs, question=question)\n",
        "print(f'Answer: {answer}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EeGieXpz-Ud",
        "outputId": "eda555fc-d13f-4c22-8f55-58c09c05007d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Continued emissions will further affect all major climate system components and lead to irreversible changes on centennial to millennial time scales. These changes will become larger with increasing global warming. Without urgent mitigation and adaptation actions, climate change will increasingly threaten ecosystems, biodiversity, livelihoods, and the health and well-being of current and future generations. Continued emissions will also intensify the global water cycle, including its variability, global monsoon precipitation, and extreme weather events. Additionally, economic damages have already been detected in climate-exposed sectors, such as agriculture, forestry, fishery, energy, and tourism. Adverse effects on human health, livelihoods, and infrastructure, especially in urban areas, have also been observed due to climate change.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's try to ask a question that goes beyond the information provided in the document."
      ],
      "metadata": {
        "id": "FvyBgg1y0BSy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scPsEEisVWCc",
        "outputId": "dfc80758-32c1-488b-94cc-66b62b0e57b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: I don't know the answer to that question.\n"
          ]
        }
      ],
      "source": [
        "# Search for similar text in the document\n",
        "question_out_of_context = \"How many oscars did Titanic win?\"\n",
        "docs = knowledge_base.similarity_search(question_out_of_context, top_k=3)\n",
        "\n",
        "# Use similar text to give ChatGPT some context\n",
        "answer = chain.run(input_documents=docs, question=question_out_of_context)\n",
        "print(f'Answer: {answer}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uHHZDfXShTI"
      },
      "source": [
        "# Review Cost\n",
        "\n",
        "In this section, we will review the costs of the API call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "y5odxuTCSqGU"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks import get_openai_callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haQ9vT0qTR5v",
        "outputId": "da7ce93f-263c-409b-e149-9139696b979c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens Used: 1000\n",
            "\tPrompt Tokens: 814\n",
            "\tCompletion Tokens: 186\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.0015929999999999998\n"
          ]
        }
      ],
      "source": [
        "with get_openai_callback() as cb:\n",
        "    answer = chain.run(input_documents=docs, question=question)\n",
        "    print(cb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}